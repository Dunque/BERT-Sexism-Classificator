I've seen some models of bert which specialise in the classification of tweets. So I decided to try it to see if it
outperforms the base bert model that I have right now.

It uses Roberta as a base, with a few differences over the regular bert.
https://arxiv.org/abs/1907.11692

Here is the Bertweet paper:
https://aclanthology.org/2020.emnlp-demos.2.pdf

First off, i removed the text preprocessing that i used for the base bert, and switched to the recommended TweetNormalizer
library

Start training...

 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   1    |   20    |   0.681526   |     -      |     -     |   3.90
   1    |   40    |   0.637748   |     -      |     -     |   3.72
   1    |   60    |   0.558585   |     -      |     -     |   3.72
   1    |   80    |   0.588075   |     -      |     -     |   3.73
   1    |   100   |   0.586691   |     -      |     -     |   3.73
   1    |   120   |   0.573455   |     -      |     -     |   3.74
   1    |   140   |   0.537893   |     -      |     -     |   3.74
   1    |   160   |   0.513451   |     -      |     -     |   3.74
   1    |   180   |   0.483882   |     -      |     -     |   3.75
   1    |   196   |   0.494019   |     -      |     -     |   2.90
----------------------------------------------------------------------
   1    |    -    |   0.567573   |  0.449553  |   80.45   |   36.68
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           1     0.8017    0.8153    0.8085       352
           0     0.8088    0.7948    0.8017       346

    accuracy                         0.8052       698
   macro avg     0.8052    0.8051    0.8051       698
weighted avg     0.8052    0.8052    0.8051       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   2    |   20    |   0.455971   |     -      |     -     |   4.00
   2    |   40    |   0.462661   |     -      |     -     |   3.76
   2    |   60    |   0.396247   |     -      |     -     |   3.76
   2    |   80    |   0.374873   |     -      |     -     |   3.77
   2    |   100   |   0.416006   |     -      |     -     |   3.77
   2    |   120   |   0.439929   |     -      |     -     |   3.77
   2    |   140   |   0.415287   |     -      |     -     |   3.77
   2    |   160   |   0.437754   |     -      |     -     |   3.77
   2    |   180   |   0.405887   |     -      |     -     |   3.77
   2    |   196   |   0.425649   |     -      |     -     |   2.92
----------------------------------------------------------------------
   2    |    -    |   0.423140   |  0.406639  |   81.62   |   37.05
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           1     0.8256    0.8068    0.8161       352
           0     0.8079    0.8266    0.8171       346

    accuracy                         0.8166       698
   macro avg     0.8167    0.8167    0.8166       698
weighted avg     0.8168    0.8166    0.8166       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   3    |   20    |   0.301767   |     -      |     -     |   3.97
   3    |   40    |   0.273763   |     -      |     -     |   3.78
   3    |   60    |   0.275640   |     -      |     -     |   3.77
   3    |   80    |   0.323988   |     -      |     -     |   3.77
   3    |   100   |   0.296650   |     -      |     -     |   3.78
   3    |   120   |   0.297468   |     -      |     -     |   3.78
   3    |   140   |   0.299998   |     -      |     -     |   3.78
   3    |   160   |   0.249070   |     -      |     -     |   3.78
   3    |   180   |   0.295981   |     -      |     -     |   3.78
   3    |   196   |   0.305659   |     -      |     -     |   2.92
----------------------------------------------------------------------
   3    |    -    |   0.291770   |  0.410275  |   84.21   |   37.10
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           1     0.8418    0.8466    0.8442       352
           0     0.8430    0.8382    0.8406       346

    accuracy                         0.8424       698
   macro avg     0.8424    0.8424    0.8424       698
weighted avg     0.8424    0.8424    0.8424       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   4    |   20    |   0.210608   |     -      |     -     |   3.98
   4    |   40    |   0.193729   |     -      |     -     |   3.76
   4    |   60    |   0.172390   |     -      |     -     |   3.77
   4    |   80    |   0.207490   |     -      |     -     |   3.77
   4    |   100   |   0.177261   |     -      |     -     |   3.77
   4    |   120   |   0.184150   |     -      |     -     |   3.77
   4    |   140   |   0.178206   |     -      |     -     |   3.78
   4    |   160   |   0.233646   |     -      |     -     |   3.78
   4    |   180   |   0.186293   |     -      |     -     |   3.78
   4    |   196   |   0.207369   |     -      |     -     |   2.92
----------------------------------------------------------------------
   4    |    -    |   0.194944   |  0.449234  |   83.50   |   37.07
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           1     0.8475    0.8210    0.8341       352
           0     0.8235    0.8497    0.8364       346

    accuracy                         0.8352       698
   macro avg     0.8355    0.8354    0.8352       698
weighted avg     0.8356    0.8352    0.8352       698



Training complete!
AUC: 0.9011
Accuracy: 83.52%

It worked like a charm, improving the base bert by 3,5%. It lost some accuracy over the fourth epoch, so it still needs
a bit more fine tuning. Overall great results.

