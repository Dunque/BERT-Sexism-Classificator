I will make the spanish classifier using spanberta, a bert specialized in the spanish language. I will try first the
binary classifier to check if the results are indeed better thant the regular uncased bert.

I will start by replacing the necessary parameters to make it work with the code that  I already have

https://chriskhanhtran.github.io/posts/spanberta-bert-for-spanish-from-scratch/#1-install-dependencies

We pretrained SpanBERTa on OSCAR's Spanish corpus. The full size of the dataset is 150 GB and we used a portion
of 18 GB to train.

In this example, for simplicity, we will use a dataset of Spanish movie subtitles from OpenSubtitles.
This dataset has a size of 5.4 GB and we will train on a subset of ~300 MB.


The original BERT implementation uses a WordPiece tokenizer with a vocabulary of 32K subword units. This method,
however, can introduce "unknown" tokens when processing rare words.

In this implementation, we use a byte-level BPE tokenizer with a vocabulary of 50,265 subword units (same as
RoBERTa-base). Using byte-level BPE makes it possible to learn a subword vocabulary of modest size that can encode
any input without getting "unknown" tokens.

Because ByteLevelBPETokenizer produces 2 files ["vocab.json", "merges.txt"] while BertWordPieceTokenizer produces
only 1 file vocab.txt, it will cause an error if we use BertWordPieceTokenizer to load outputs of a BPE tokenizer.


1. Model Architecture

RoBERTa has exactly the same architecture as BERT. The only differences are:

    RoBERTa uses a Byte-Level BPE tokenizer with a larger subword vocabulary (50k vs 32k).
    RoBERTa implements dynamic word masking and drops next sentence prediction task.
    RoBERTa's training hyperparameters.

Other architecture configurations can be found in the documentation (RoBERTa, BERT)


2. Training Hyperparameters

Note the batch size when training RoBERTa is 8000. Therefore, although RoBERTa-base was trained for 500K steps,
its training computational cost is 16 times that of BERT-base. In the RoBERTa paper, it is shown that training with
large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.
Larger batch size can be obtained by tweaking gradient_accumulation_steps.