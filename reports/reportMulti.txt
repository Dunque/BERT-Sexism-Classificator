There are two ways of extending simple classifiers to do multi class classification:

Source Wikipedia

The first one is called One-vs.-rest strategy. It involves training a single classifier per class, 
with the samples of that class as positive samples and all other samples as negatives. 
This strategy requires the base classifiers to produce a real-valued confidence score for its decision. 
During inference, you give a sample to each model, retrieve the probabilities of belonging to the positive 
class and chose the class where the classifier is most confident.

The second way is called one-vs.-one (OvO) reduction, one trains K (K − 1) / 2 binary classifiers for a 
K-way multiclass problem; each receives the samples of a pair of classes from the original training set, 
and must learn to distinguish these two classes. At prediction time, a voting scheme is applied: 
all K (K − 1) / 2 classifiers are applied to an unseen sample and the class that got the highest number 
of "+1" predictions gets predicted by the combined classifier. This approach can lead to ambiguity in some cases.

I would recommend using One vs Rest. It is already implemented in some packages such as Sklearn

http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html


i will try one versus rest, as it seems the most appropiate for my problem.


BERT -----------------------------------------------------------------------------

TO start with the multi-class classificator, i'm taking the binary one as a baseline.

First, i translate the tags to numbers, and sotre them in a dictionary to later transform them back.

I plot the label distribution, and it shows that half the tweets are non-sexist. This matches the
half sexist half non sexist distribution of the binary classification, but now we face a new problem:
Our dataset has been severly reduced, as we only get roughly 1/5 of half of the dataset for each label.

Also, to show the results, i a m also going to plot a ROC, but this time i need to adapt it to a multiclass classifier
I'll follow the official documentation shown here:
https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#multiclass-settings


As i adapted the roc plot for the multiclass clasifier, now i'm getting a cuda error. It might be due to this
https://stackoverflow.com/questions/51691563/cuda-runtime-error-59-device-side-assert-triggered

It was resolved after changing the number of labels from 2 to 6 in the line 

        super(BertClassifier, self).__init__()
        # Specify hidden size of BERT, hidden size of our classifier, and number of labels
        D_in, H, D_out = 768, 50, 6

Changed the way the metrics are displayed, in order to see them adapted to each class
This is the new report:

 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed 
----------------------------------------------------------------------
   1    |   20    |   1.531213   |     -      |     -     |   2.81   
   1    |   40    |   1.348175   |     -      |     -     |   2.68   
   1    |   60    |   1.242131   |     -      |     -     |   2.69   
   1    |   80    |   1.191993   |     -      |     -     |   2.69   
   1    |   100   |   1.117195   |     -      |     -     |   2.69   
   1    |   120   |   1.093309   |     -      |     -     |   2.69   
   1    |   140   |   1.064244   |     -      |     -     |   2.70   
   1    |   160   |   0.979898   |     -      |     -     |   2.70   
   1    |   180   |   0.965570   |     -      |     -     |   2.70   
   1    |   196   |   0.944086   |     -      |     -     |   2.09   
----------------------------------------------------------------------
   1    |    -    |   1.153864   |  0.942012  |   66.25   |   26.43  
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.7079    0.8757    0.7829       346
           1     0.7209    0.6263    0.6703        99
           2     0.4483    0.2364    0.3095        55
           3     0.5294    0.5625    0.5455        48
           4     0.5849    0.3444    0.4336        90
           5     0.5098    0.4333    0.4685        60

    accuracy                         0.6619       698
   macro avg     0.5835    0.5131    0.5350       698
weighted avg     0.6442    0.6619    0.6412       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed 
----------------------------------------------------------------------
   2    |   20    |   0.837719   |     -      |     -     |   2.99   
   2    |   40    |   0.736027   |     -      |     -     |   2.89   
   2    |   60    |   0.747189   |     -      |     -     |   2.73   
   2    |   80    |   0.772422   |     -      |     -     |   2.76   
   2    |   100   |   0.805468   |     -      |     -     |   2.84   
   2    |   120   |   0.783594   |     -      |     -     |   2.87   
   2    |   140   |   0.703495   |     -      |     -     |   2.74   
   2    |   160   |   0.755850   |     -      |     -     |   2.75   
   2    |   180   |   0.718206   |     -      |     -     |   2.74   
   2    |   196   |   0.744953   |     -      |     -     |   2.13   
----------------------------------------------------------------------
   2    |    -    |   0.761200   |  0.882772  |   67.53   |   27.43  
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.8123    0.8006    0.8064       346
           1     0.6404    0.7374    0.6854        99
           2     0.4872    0.3455    0.4043        55
           3     0.5385    0.5833    0.5600        48
           4     0.4943    0.4778    0.4859        90
           5     0.4769    0.5167    0.4960        60

    accuracy                         0.6748       698
   macro avg     0.5749    0.5769    0.5730       698
weighted avg     0.6736    0.6748    0.6726       698

the non_sexist label seems to outperform the others, as it was expected, due to its larger presence in the dataset.

I tried with 4 epochs, and while some labels get classified better, others suffer a bit in their precision.
Adding more epochs doesn't seem like a good fit, so I'll try other fine-tuning techniques

 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   1    |   20    |   1.531867   |     -      |     -     |   3.79
   1    |   40    |   1.347120   |     -      |     -     |   3.69
   1    |   60    |   1.226254   |     -      |     -     |   3.57
   1    |   80    |   1.182856   |     -      |     -     |   3.68
   1    |   100   |   1.119234   |     -      |     -     |   3.70
   1    |   120   |   1.089800   |     -      |     -     |   3.69
   1    |   140   |   1.061078   |     -      |     -     |   3.72
   1    |   160   |   0.976074   |     -      |     -     |   3.64
   1    |   180   |   0.967843   |     -      |     -     |   3.66
   1    |   196   |   0.949276   |     -      |     -     |   2.92
----------------------------------------------------------------------
   1    |    -    |   1.151080   |  0.937806  |   66.47   |   36.08
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.7170    0.8642    0.7837       346
           1     0.7381    0.6263    0.6776        99
           2     0.4857    0.3091    0.3778        55
           3     0.5098    0.5417    0.5253        48
           4     0.5763    0.3778    0.4564        90
           5     0.5000    0.4333    0.4643        60

    accuracy                         0.6648       698
   macro avg     0.5878    0.5254    0.5475       698
weighted avg     0.6507    0.6648    0.6493       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   2    |   20    |   0.826165   |     -      |     -     |   3.80
   2    |   40    |   0.717560   |     -      |     -     |   3.52
   2    |   60    |   0.746188   |     -      |     -     |   3.52
   2    |   80    |   0.757771   |     -      |     -     |   3.52
   2    |   100   |   0.788783   |     -      |     -     |   3.53
   2    |   120   |   0.784819   |     -      |     -     |   3.54
   2    |   140   |   0.729017   |     -      |     -     |   3.61
   2    |   160   |   0.763176   |     -      |     -     |   3.58
   2    |   180   |   0.714872   |     -      |     -     |   3.60
   2    |   196   |   0.745509   |     -      |     -     |   2.74
----------------------------------------------------------------------
   2    |    -    |   0.757976   |  0.894974  |   68.20   |   34.96
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.7973    0.8410    0.8186       346
           1     0.7195    0.5960    0.6519        99
           2     0.5833    0.3818    0.4615        55
           3     0.6389    0.4792    0.5476        48
           4     0.4320    0.6000    0.5023        90
           5     0.5185    0.4667    0.4912        60

    accuracy                         0.6819       698
   macro avg     0.6149    0.5608    0.5789       698
weighted avg     0.6874    0.6819    0.6793       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   3    |   20    |   0.519621   |     -      |     -     |   3.86
   3    |   40    |   0.512951   |     -      |     -     |   3.70
   3    |   60    |   0.468622   |     -      |     -     |   3.63
   3    |   80    |   0.493447   |     -      |     -     |   3.56
   3    |   100   |   0.475918   |     -      |     -     |   3.57
   3    |   120   |   0.524653   |     -      |     -     |   3.61
   3    |   140   |   0.445156   |     -      |     -     |   3.60
   3    |   160   |   0.519910   |     -      |     -     |   3.58
   3    |   180   |   0.456141   |     -      |     -     |   3.58
   3    |   196   |   0.449839   |     -      |     -     |   2.76
----------------------------------------------------------------------
   3    |    -    |   0.487540   |  0.954606  |   66.50   |   35.46
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.8024    0.7630    0.7822       346
           1     0.6355    0.6869    0.6602        99
           2     0.5111    0.4182    0.4600        55
           3     0.5455    0.6250    0.5825        48
           4     0.5161    0.5333    0.5246        90
           5     0.4493    0.5167    0.4806        60

    accuracy                         0.6648       698
   macro avg     0.5767    0.5905    0.5817       698
weighted avg     0.6709    0.6648    0.6666       698



 Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed
----------------------------------------------------------------------
   4    |   20    |   0.295386   |     -      |     -     |   4.24
   4    |   40    |   0.264820   |     -      |     -     |   4.08
   4    |   60    |   0.276635   |     -      |     -     |   3.58
   4    |   80    |   0.338508   |     -      |     -     |   4.02
   4    |   100   |   0.313571   |     -      |     -     |   3.64
   4    |   120   |   0.325047   |     -      |     -     |   3.58
   4    |   140   |   0.265570   |     -      |     -     |   3.63
   4    |   160   |   0.293596   |     -      |     -     |   3.69
   4    |   180   |   0.283161   |     -      |     -     |   3.67
   4    |   196   |   0.272058   |     -      |     -     |   2.87
----------------------------------------------------------------------
   4    |    -    |   0.293270   |  1.027199  |   68.55   |   36.99
----------------------------------------------------------------------
Classification Report:
              precision    recall  f1-score   support

           0     0.7826    0.8324    0.8067       346
           1     0.6600    0.6667    0.6633        99
           2     0.5333    0.4364    0.4800        55
           3     0.6222    0.5833    0.6022        48
           4     0.5405    0.4444    0.4878        90
           5     0.4848    0.5333    0.5079        60

    accuracy                         0.6848       698
   macro avg     0.6039    0.5828    0.5913       698
weighted avg     0.6777    0.6848    0.6798       698


I will try some dataset balancing techniques in order to improve these results.
https://d1wqtxts1xzle7.cloudfront.net/60427831/poster_unbalanced-with-cover-page-v2.pdf?Expires=1655208946&Signature=RT0O3fTK35yTQR4mDo3Fil0WsaeIjOBsvfB0tHmg7NV2hroJFHlOQaaeg5pUjV--zWYGN3dqxir15Kc7ZU5Rbn3FU5o19d6PnsPhaWRCoood~UhhyAwCb0AqyTnT4Wi6AGJeXbFaeSJ5ob7ekXkrnhLcYD858R0-dCT8mmd00TqiNRLLyGX~8xPjy5dsFrWOLqeqSulC~JhvVepTR55FtJm5WwAxkJJBAiHHvlQBEP0cM~lzG9Q-KR2u~l50~B5mESMb8w7J3lKlGqn3vZ7Q36FeyVl97xy2aahULqFLQWKwQH7ehScsLHqgXSaFNuhWZMNXRL4qskuhhEgWc6aQ5w__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA
https://www.scirp.org/pdf/JILSA_2015111114204642.pdf

OVer sampling and under sampling
https://www.jmlr.org/papers/volume18/16-365/16-365.pdf


